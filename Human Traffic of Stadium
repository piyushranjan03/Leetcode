from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DateType,StringType
from pyspark.sql.functions import to_date,lead,coalesce,row_number,count,desc,col
from pyspark.sql.window import Window

# Initialize a Spark session
spark = SparkSession.builder.appName("StadiumData").getOrCreate()

# Define the schema for the DataFrame
schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("visit_date", StringType(), False),
    StructField("no_of_people", IntegerType(), False)
])

# Define the data as a list of rows
data = [
    (1, '2017-07-01', 10),
    (2, '2017-07-02', 109),
    (3, '2017-07-03', 150),
    (4, '2017-07-04', 99),
    (5, '2017-07-05', 145),
    (6, '2017-07-06', 1455),
    (7, '2017-07-07', 199),
    (8, '2017-07-08', 188)
]

# Create a DataFrame from the data and schema
stadium_df = spark.createDataFrame(data, schema=schema)
stadium_df = stadium_df.withColumn("visit_date",to_date(stadium_df["visit_date"]))

window_spec = Window.orderBy(stadium_df["visit_date"])
stadium_df = stadium_df.filter(stadium_df["no_of_people"]>=100)
# Add the lead column
stadium_df.show()
stadium_df = stadium_df.withColumn("row_num", row_number().over(window_spec))
stadium_df = stadium_df.withColumn("group",stadium_df["id"]-stadium_df["row_num"])
stadium_df_cnt=stadium_df.groupBy('group').agg(count('*'))
stadium_df_cnt.show()

stadium_df_final=stadium_df.join(stadium_df_cnt,stadium_df.group==stadium_df_cnt.group,'inner')
df=stadium_df_final.filter(stadium_df_final["count(1)"]>=3).select('id','visit_date','no_of_people').orderBy(col('id').desc())
df.show()

                                                                                                                   
